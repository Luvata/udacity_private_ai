{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện mạng Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NN với non-linear activation sau khi huấn luyện sẽ xấp xỉ được hàm mapping từ input -> output ( từ ảnh -> chữ số trong ảnh)\n",
    "- Huấn luyện: \n",
    "    - Input: real data (dạng Tensor)\n",
    "    - Forward -> Prediction (dự đoán)\n",
    "    - Loss function : Độ tốt/ kém của dự đoán đó (MSE, NLL, BCE...)\n",
    "    - Backpropagtion (or chain-rule): Tính $\\nabla W = \\frac{\\partial Loss}{\\partial W}$\n",
    "    - Cập nhật tham số $W \\leftarrow \\alpha * \\nabla W$ \n",
    ">  You can think of this like descending a mountain by following the steepest slope to the base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain-rule\n",
    "`$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "Gradient desent step\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses trong Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.CrossEntropyLoss` = `nn.LogSoftmax()` & `nn.NLLLoss()`\n",
    "\n",
    "Input của `nn.CrossEntropyLoss` là `logits` - kết quả của X.dot(W), chưa đi qua softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3133, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tác giả gợi ý nên dùng `nn.LogSoftmax()` hoặc `F.log_softmax` đi cùng với `nn.NLLLoss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss. Note that for nn.LogSoftmax and F.log_softmax you'll need to set the dim keyword argument appropriately. dim=0 calculates softmax across the rows, so each column sums to 1, while dim=1 calculates across the columns so each row sums to 1. Think about what you want the output to be and choose dim appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3012, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# TODO: Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "### Run this to check your work\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autograd: Tự động tính gradients cho tensors\n",
    "- Cách hoạt động: Theo dõi từng phép toán của tensors, backwards ngược lại qua các phép toán đó và tính gradient (eg: Define hàm `__add__` với parents node)\n",
    "- Sử dụng Pytorch autograd để tính $\\frac{\\partial Loss}{\\partial W}$ \n",
    "- `x.requires_grad_(True)` hoặc set `requires_grad = True` cho 1 tensor để đảm bảo Autograd \"keep track\" tensor đó. \n",
    "- Turn off gradient cho 1 block sử dụng `torch.no_grad()` (Thường sử dụng trong lúc test),\n",
    "- Cho tất cả: `torch.set_grad_enabled(True|False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0973, -0.0286],\n",
      "        [-0.0226, -1.3194]], requires_grad=True) \n",
      " tensor([[9.4695e-03, 8.1808e-04],\n",
      "        [5.0977e-04, 1.7407e+00]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "y = x**2\n",
    "print(x, \"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4379, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0487, -0.0143],\n",
      "        [-0.0113, -0.6597]])\n",
      "tensor([[-0.0487, -0.0143],\n",
      "        [-0.0113, -0.6597]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tính gradients, gọi method `backward`. Khi z gọi `backward`, `x.grad` sẽ là $\\frac{\\partial z}{\\partial x}$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Autograd together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong pytorch, mọi parameters được khởi tạo với requires_grad=True\n",
    "- Khi `loss.backward()`, gradients cho từng parameter được tính, và sử dụng cho update weights với gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0020,  0.0020,  0.0020,  ...,  0.0020,  0.0020,  0.0020],\n",
      "        [-0.0004, -0.0004, -0.0004,  ..., -0.0004, -0.0004, -0.0004],\n",
      "        [ 0.0020,  0.0020,  0.0020,  ...,  0.0020,  0.0020,  0.0020],\n",
      "        ...,\n",
      "        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010],\n",
      "        [-0.0009, -0.0009, -0.0009,  ..., -0.0009, -0.0009, -0.0009]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network!\n",
    "\n",
    "Sử dụng các optimizers trong `optim`, vd `otpim.SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi gọi `loss.backward()` qua nhiều lần với cùng 1 bộ parameters, gradients bị cộng dồn\n",
    "- Sử dụng `optimizer.zero_grad()` để đưa gradient về 0 sau mỗi training pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0149, -0.0294, -0.0160,  ..., -0.0341,  0.0111,  0.0048],\n",
      "        [-0.0087,  0.0227, -0.0271,  ..., -0.0281, -0.0315,  0.0293],\n",
      "        [-0.0005,  0.0068,  0.0036,  ..., -0.0192, -0.0062, -0.0317],\n",
      "        ...,\n",
      "        [-0.0343,  0.0108, -0.0170,  ..., -0.0064, -0.0196, -0.0098],\n",
      "        [-0.0223, -0.0111, -0.0253,  ..., -0.0086,  0.0131,  0.0311],\n",
      "        [-0.0285,  0.0054,  0.0023,  ...,  0.0112,  0.0087, -0.0250]],\n",
      "       requires_grad=True)\n",
      "Gradient -\n",
      " tensor([[ 2.6238e-03,  2.6238e-03,  2.6238e-03,  ...,  2.6238e-03,\n",
      "          2.6238e-03,  2.6238e-03],\n",
      "        [-7.4986e-05, -7.4986e-05, -7.4986e-05,  ..., -7.4986e-05,\n",
      "         -7.4986e-05, -7.4986e-05],\n",
      "        [-1.3641e-04, -1.3641e-04, -1.3641e-04,  ..., -1.3641e-04,\n",
      "         -1.3641e-04, -1.3641e-04],\n",
      "        ...,\n",
      "        [ 1.3588e-03,  1.3588e-03,  1.3588e-03,  ...,  1.3588e-03,\n",
      "          1.3588e-03,  1.3588e-03],\n",
      "        [ 7.0200e-04,  7.0200e-04,  7.0200e-04,  ...,  7.0200e-04,\n",
      "          7.0200e-04,  7.0200e-04],\n",
      "        [-5.3723e-05, -5.3723e-05, -5.3723e-05,  ..., -5.3723e-05,\n",
      "         -5.3723e-05, -5.3723e-05]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -\\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0148, -0.0294, -0.0160,  ..., -0.0342,  0.0110,  0.0048],\n",
      "        [-0.0087,  0.0227, -0.0271,  ..., -0.0281, -0.0315,  0.0293],\n",
      "        [-0.0005,  0.0068,  0.0036,  ..., -0.0192, -0.0062, -0.0317],\n",
      "        ...,\n",
      "        [-0.0343,  0.0108, -0.0170,  ..., -0.0065, -0.0196, -0.0098],\n",
      "        [-0.0223, -0.0111, -0.0253,  ..., -0.0086,  0.0131,  0.0311],\n",
      "        [-0.0285,  0.0054,  0.0023,  ...,  0.0112,  0.0087, -0.0250]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exercise: Implement the training pass for our network. If you implemented it correctly, you should see the training loss drop with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9188730010091624\n",
      "Training loss: 0.854012284070444\n",
      "Training loss: 0.5255496400569294\n",
      "Training loss: 0.43114745340494715\n",
      "Training loss: 0.3857703159517571\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # TODO: Training pass\n",
    "        output = model(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
